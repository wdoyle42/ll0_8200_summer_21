---
title: "Cross Validation"
author: "Will Doyle"
output: html_document
---

## Introduction

The essence of prediction is discovering the extent to which our models can predict outcomes for data that does not come from our sample. Many times this process is temporal. We fit a model to data from one time period, then take predictors from a subsequent time period to come up with a prediction in the future. For instance, we might use data on team performance to predict the likely winners and losers for upcoming soccer games. 

This process does not have to be temporal. We can also have data that is out of sample because it hadn't yet been collected when our first data was collected, or we can also have data that is out of sample because we designated it as out of sample.

The data that is used to generate our predictions is known as 
*training* data. The idea is that this is the data used to train our model, to let it know what the relationship is between our predictors and our outcome. So far, we have worked mostly with training data. 

That data that is used to validate our predictions is known as *testing* data. With testing data, we take our trained model and see how good it is at predicting outcomes using out of sample data. 

One very simple approach to this would be to cut our data in half. This is what we've done so far.  We could then train our model on half the data, then test it on the other half. This would tell us whether our measure of model fit (e.g. rmse, auc) is similar or different when we apply our model to out of sample data. 

But this would only be a "one-shot" approach. It would be better to do this multiple times, cutting the data into two parts: training and testing, then fitting the model to the training data, and then checking its predictions against the testing data. That way, we could generate a large number of rmse's to see how well the model fits on lots of different possible out-of-sample predictions. 

This process is called *cross-fold validation*, and it involves two important decisions: first, how will the data be cut, and how many times will the validation run. 

```{r}
library(tidyverse)
library(tidymodels)
library(glmnet)
library(modelr)
```

Next we load the quickfacts data, which contains county-by-county information. We're going to create a simple model that predicts median home values in the county as a function of education, home ownership and income. 

```{r}
load("pd.Rdata")
pd<-pd%>%
  select(median_home_val,median_hh_inc,coll_grad_pc,homeown_rate,per_capita_inc,pop65p,retail_percap)%>%
  mutate_all(.funs=list(as.numeric)) ## change all to numeric

```

A quick look at this outcome lets us know it needs to be logged. 
```{r}
pd%>%
ggplot(aes(x=coll_grad_pc,y=median_home_val))+
geom_point()
```


Plotting the variable with a log transformation shows that it has a more symmetric distribution using this transformation. 

```{r}
pd%>%
ggplot(aes(x=coll_grad_pc,y=median_home_val))+
geom_point()+
scale_y_continuous(trans="log")
```


## Set the kind of model to run

We're going to follow the `tidymodels` approach and set the elements of our workflow. First up is the type of model-- a linear regression. 

```{r}
lm_fit <- 
  linear_reg() %>% 
  set_engine("lm")
```

## Define the model

Next we'll define the terms in the model. We'll use education homeownership rate and income to predict the median home value in a county. 
```{r}
lm_formula<-as.formula("median_home_val~
                        coll_grad_pc+
                        per_capita_inc+
                        homeown_rate+
                        median_hh_inc")
```

## Define a recipe

For the recipe we're going to include a few steps. `step_log` will log transform the outcome, `step_zv` will remove any variable that are zero variance (constants) and `step_naomit` will drop any missing data. 

```{r}
lm_rec <- recipe(lm_formula, data = pd) %>%
  step_log(all_outcomes())%>%
  step_zv(all_numeric()) %>% # drop any zero variance
  step_naomit(all_predictors()) ## drop any missing data

```

## Define Workflow

Now we can put the recipe and the model together into the workflow. 

```{r}
lm_workflow<-workflow()%>%
  add_recipe(lm_rec)%>%
  add_model(lm_fit)
```



## Specify the resampling: K fold resampling, K=10

Now we're at the new part. Notice that we didn't spli the data testing and training this time. Instead we're going to specify that we want to spit the data using into 10 different folds. Each fold wil split the data with 9/10 training and 1/10 testing. 
```{r}
folds <- vfold_cv(pd, v = 10)
```


## Fit Model and Cross Validate

To fit the model to the data, we use `fit_resample`. We need to specify the workflow, the resampled data (which we created above) and the control function, which tells R what to save. In this case, the `control_resamples` tells R to save the predictions from the model. 

```{r}
lm_kfold_results<-
  fit_resamples(
    lm_workflow, ## Recipe: preps the data
    folds, ##resampling plan
    control=control_resamples(save_pred = TRUE)
  )
```


## Assessing Accuracy

Now we can pull the results using `collect_metrics`. Instead of a single rmse, we will have 10 rmses, one for each fold. 

```{r}
lm_kfold_results%>%
  collect_metrics()
```

The next step "unnests" the data so we can work with it. 

```{r}
lm_kfold_results%>%
  unnest(.metrics)
```



Now we can plot the results.

## RMSE from 10 fold cross-validation

```{r}
lm_kfold_results%>%
  unnest(.metrics)%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate))+
  geom_density()
```

This shows that in our 10-fold cross validation, the rmse went from a minimum of abourt .27 to a maximum of about .32.

## Specify the resampling: monte_carlo resampling

To change to mont carlo (or random partition) resampling, we need only change how the resampling is specified. The code below creates 100 datasets, with 8/10 set aside for training in each resample, and 2/10 (`prop=.2`) set aside for testing. 

```{r}
pd_mc_rs<-mc_cv(pd,times = 100,prop = .8) ##1000 is usual minimum
```

Nothing else needs to change, we'll just fit our previously specified workflow to the data that's set up for monte carlo resampling. 

## Fit Monte Carlo Resampling
```{r}
lm_mc_results<-
  fit_resamples(
    lm_workflow,
    pd_mc_rs, ##resampling plan
    control=control_resamples(save_pred = TRUE)
  )
```

The same process will happen below-- we can plot the results from the 100 different applications to the testing datasets to see what the accuracy of the model is under a variety of different testing/training splits. 

## Get Metrics
```{r}
lm_mc_results%>%
  collect_metrics()
```

The overall average rmse is .294, and the average $r^2$ is .606. 

## Plot Monte Carlo resampling results
```{r}
lm_mc_results%>%
  unnest(.metrics)%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate))+
  geom_density()
```

With more resamples, we can see that the minimum rmse is still about .27 and the maximum is about .32. 

## Feature Selection

Of course, we can also just let the computer choose a model from a set of candidate variables. Below, I use lasso regression, which involves proposing candidate variables and evaluating their ability to lower RMSE, as the basis for choosing a "best" model. 

## Set Lasso Formula

The formula for this model will be `y~.` which means run the model wuth ALL of the variables in the dataset. 

```{r}
lasso_formula<-as.formula("median_home_val~.")
```

## Set Lasso Recipe

I'll use the same recipe as above to process this data. 
```{r}
lasso_rec <- recipe(lasso_formula, data = pd) %>%
  step_log(all_outcomes())%>%
  step_zv(all_numeric()) %>% # drop any zero variance
  step_naomit(all_predictors()) ## drop any missing data
```


## Specify Lasso Model

We're going to use a different model, which needs to have two parameters set. These are the penalty and the mixture. I'm going to use a standard value of .1 for the penalty and 1 for the mixture (this makes it a lasso model). Notice that the engine is now "glmnet."

```{r}
penalty_spec<-.1
mixture_spec<-1

lasso_fit<- 
  linear_reg(penalty=penalty_spec,
             mixture=mixture_spec) %>% 
  set_engine("glmnet")
```


Next I put the recipe and model into the workflow. 
```{r}
lasso_wf<-workflow()%>%
  add_recipe(lasso_rec)%>%
  add_model(lasso_fit)
```


## Fit Monte Carlo Resamples from Lasso

I don't need to respecify the sampling plan, because we'll use the monte carlo set up again.
```{r}
lasso_mc_results<-
  fit_resamples(
    lasso_wf, ## Recipe: preps the data
    pd_mc_rs, ##resampling plan
    control=control_resamples(save_pred = TRUE)
  )
```

Once we've run the model we can plot the accuracy of the rmses from the 100 different resamples. 

```{r}
lasso_mc_results%>%
  unnest(.metrics)%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate))+
  geom_density()
```


## Comparing Performance of Models

At this point we've fit a model using a standard linear regression and a model that allowed R to use all of the variables in the dataset. Let's figure out which one worked better. I'm going to combine the metrics from both datasets into a single dataset, then plot the results.
```{r}
lm_mc_results%>%
  unnest(.metrics)%>%
  mutate(model="lm")%>%
  bind_rows(lasso_mc_results%>%
              unnest(.metrics)%>%
              mutate(model="lasso")
              )%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate,fill=model))+
  geom_density(alpha=.5)
```

In this case our nice simple linear model outperformed the more complex model, and would be preferred. This happens! 

## Cross Validation for Classification

The process of cross validatin in classification is VERY similar. We need to specify our resampling plan then fit the model to the resample data. 

We'll use the pizza data again and make sure to have the dependent variable structured as a factor. I'm going to drop the original version of the `got_pizza` variable as it wil obviously perfectly predict the outcome. 

```{r}
load("za.RData")

za<-za%>%
  drop_na()%>%
  mutate(got_pizza_f=fct_relevel(got_pizza_f,"Yes","No"))%>%
  select(-got_pizza)
```

## Set formula for classification

I'll use a model similar to the one we used in our lesson to predict whether or not someone received a pizza. 

```{r}
#  Model terms
za_formula<-as.formula("got_pizza_f~
             age+
             karma+
             total_posts+
             raop_posts+
             student+
             grateful+
             pop_request+
             score")
```

## Prep Recipe for Classification

Next we'll get the recipe set up. Notice the `step_dummy` and the `step_naomit` are applied to types of variables--step_dummy is applied to `all_nominal` variables but not `all_outcomes` (becuase of the minus sigh), while `step_naomit` is applied to both `all_predictors` and `all_outcomes`.


```{r}
logit_rec <- recipe(za_formula, data = za) %>%
  step_zv(all_numeric()) %>% # drop any zero variance
  step_dummy(all_nominal(),-all_outcomes())%>%
  step_naomit(all_predictors(),all_outcomes()) 
```


## Specify Model

We'll specify that we want a model for classification, and we'll use `glm` as our engine. 

```{r}
logit_fit<-
  logistic_reg(mode="classification")%>%
  set_engine("glm")
```

## Set Resampling


As above, we'll use a monte carlo approach with 100 resamples, each of which will be split 80/20 training/testing. 

```{r}
logit_mc_rs<-mc_cv(za,times=100)
```

## Creat Workflow for Logit Model

We can combine the model and recipe into our workflow. 
```{r}
logit_wf<-workflow()%>%
  add_recipe(logit_rec)%>%
  add_model(logit_fit)
```


## Fit Logit Model to Resampleld Data

And now we can fit the model to the resamples. Note that in the metric I ask for our key measures of roc_auc, accuracy, sensitivity and specificity.
```{r}
logit_mc <- 
  fit_resamples(
    logit_wf,
    logit_mc_rs,
    metrics = metric_set(roc_auc, sens, spec, accuracy)
  )
```

## Collect Metrics from Logit model
```{r}
logit_mc%>%
  collect_metrics()
```


## Plot distribution of AUC

Above we plotted the distribution of rmse for the repeated fitting of the linear model. As this is a model for classification, we'll use the AUC instead. 
```{r}
logit_mc%>%
  unnest(.metrics)%>%
  filter(.metric=="roc_auc")%>%
  ggplot(aes(x=.estimate))+
  geom_density()
```

The results show that the AUC goes from about .57 to about .63 in the testing datasets. 

## Feature Selection in Classification.

We can also do feature selection in the context of classification. As above, we're going to fit the dependent variable to ALL of the variables in the dataset, but let R make the decisions about which variables should be more important. 

```{r}
lasso_logit_formula<-as.formula("got_pizza_f~.")
```



## Set Lasso Recipe

The recipe in this case is more complex, as I need to transform mutiple variables. The basic idea is the same as previously. 
```{r}
lasso_logit_rec <- recipe(lasso_logit_formula, data = za) %>%
  step_zv(all_numeric()) %>% # drop any zero variance
  step_dummy(all_nominal(),-all_outcomes())%>%
  step_naomit(all_predictors(),all_outcomes())%>%
  step_log(total_posts,offset=1)%>%
  step_scale(all_predictors())%>%
  step_center(all_predictors())
```


## Specify Elastic Net Model Using Model Tuning

Now we're going to do something kind of different. Above I noted that these kinds of models have two parameters: penalty and mixture. In the previous example I chose the values for the penalty and the mixture. But we can also try a number of different possibiities in order to see which combination of penalty and mixture might be best. This is called "model tuning." We have two hyperparameters: penallty and mixture. I'm going to note that these will be "tuned" below. 
```{r}
lasso_logit_fit<- 
  logistic_reg(mode="classification",
             penalty=tune(),
             mixture=tune()) %>% 
  set_engine("glmnet")

```


## Create Workflow for Lasso Logit Model

I combine the recipe and the model into the workflow. 

```{r}
lasso_logit_wf<-workflow()%>%
  add_recipe(lasso_logit_rec)%>%
  add_model(lasso_logit_fit)
```


## Fit Monte Carlo Resamples from Elastic Net

Now I'm going to fit the mode to the resampled data. I have one more step-- the `grid` command. This provides a set of possible values for the penaty and the mixture. We'll pick the best model from the available settings afterwards. This is pretty complicated stuff and it wil take a minute or two to run. 
```{r}
lasso_logit_mc<-
  tune_grid(
    lasso_logit_wf, 
    resamples=logit_mc_rs, ##resampling plan
    grid=grid_regular(parameters(lasso_logit_fit,size=9)),
    metrics = metric_set(roc_auc, sens, spec, accuracy)
  )
```


```{r}
lasso_logit_mc%>%
  collect_metrics()%>%
  filter(.metric=="roc_auc")%>%
  select(penalty, mixture,mean)%>%
  arrange(-mean)
```


## Comparing Performance of Models

Now we can compare the model we specified with the model that 

```{r}
lasso_logit_mc%>%
  unnest(.metrics)%>%
  filter(.metric=="roc_auc")%>%
  mutate(tune_id=paste0("penalty=",prettyNum(penalty),
                        ", mixture=",prettyNum(mixture))) %>%
  select(tune_id,.estimate)%>%
  rename(ROC=.estimate)%>%
   bind_rows(logit_mc%>%
              unnest(.metrics)%>%
              filter(.metric=="roc_auc")%>%
              mutate(tune_id="Standard Logit")%>%
               select(tune_id,.estimate)%>%
              rename(ROC=.estimate)
              )%>%
  ggplot(aes(x=ROC,color=tune_id,fill=tune_id))+
  geom_density(alpha=.1)
```
  
  In this case, several of the models with very low penalties outperform the standard logit mode, achieving an AUC of about .65. 

In different situations we may care more about WHY something predicts an outcome, and in other situations we care more about WHETHER something predicts an outcome. The key is to be clear with yourself about what you're interested in. Model selection via stepwise regression or other algorithms is not a panacea. 
